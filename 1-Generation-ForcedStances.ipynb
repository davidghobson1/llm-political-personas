{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537dd2e5-c60f-46ed-b902-a06cd0962b35",
   "metadata": {},
   "source": [
    "# Generate Completions | Forced Stances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba010cf1-bde8-4f38-87ae-cb11d8bc4cef",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f535b",
   "metadata": {},
   "source": [
    "### 1.1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968224-e717-4a48-8515-3edf4b734886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Import helper funcs\n",
    "from Libraries.funcs import save_dict_as_json, load_dict_from_json\n",
    "from Libraries.inference import complete_local, complete_openai, complete_together, complete_openrouter, complete_groq\n",
    "from Libraries.decorators import format_prompt_for_openai, format_prompt_for_cohere, format_prompt_for_chatml, format_prompt_for_mistral, format_prompt_for_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41386da-2024-4ec7-8ec2-5e31c125632c",
   "metadata": {},
   "source": [
    "### 1.2 Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73977922",
   "metadata": {},
   "source": [
    "Select the model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f06f82-b64c-4920-bbed-f41d636bf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select the model to evaluate ---\n",
    "# model = \"CohereForAI/c4ai-command-r-plus\"\n",
    "model = 'meta-llama/llama-3-70b-instruct'\n",
    "# model = \"mistralai/miqu-70b-5_K_M\"\n",
    "# model = \"Qwen/Qwen1.5-72B-Chat\"\n",
    "# model = \"gpt-4-0125-preview\"\n",
    "\n",
    "# --- Select the inference engine for this model ---\n",
    "inference_engine = \"local\"\n",
    "# inference_engine = \"openai\"\n",
    "# inference_engine = \"together\"\n",
    "# inference_engine = \"openrouter\"\n",
    "# inference_engine = \"groq\"\n",
    "\n",
    "# Model used as a stance detector\n",
    "model_stance_detector = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DO NOT MODIFY ---\n",
    "\n",
    "# The complete function is set based on the inference engine\n",
    "if inference_engine == \"local\":\n",
    "    complete = complete_local\n",
    "elif inference_engine == \"openai\":\n",
    "    complete = complete_openai\n",
    "elif inference_engine == \"together\":\n",
    "    complete = complete_together\n",
    "elif inference_engine == \"openrouter\":\n",
    "    complete = complete_openrouter\n",
    "elif inference_engine == \"groq\":\n",
    "    complete = complete_groq\n",
    "else:\n",
    "    raise ValueError(\"Invalid inference engine\")\n",
    "\n",
    "\n",
    "# The decorator function is set based on the model\n",
    "if inference_engine == \"local\": \n",
    "    if \"cohere\" in model.lower():\n",
    "        format_prompt = format_prompt_for_cohere\n",
    "    elif \"llama\" in model.lower():\n",
    "        format_prompt = format_prompt_for_llama\n",
    "    elif \"mistral\" in model.lower():\n",
    "        format_prompt = format_prompt_for_mistral\n",
    "    elif \"qwen\" in model.lower():\n",
    "        format_prompt = format_prompt_for_chatml\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "else:\n",
    "    format_prompt = format_prompt_for_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1297cc-f6ba-41f8-9dab-71c3e3b3e6d7",
   "metadata": {},
   "source": [
    "### 1.3 Load Evaluation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5dd2f",
   "metadata": {},
   "source": [
    "We load our personas, forced stances and tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8f7c1-9a1b-4c26-96cb-ce725d0ea681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCT framework from library\n",
    "from Libraries.pct import response_options\n",
    "\n",
    "# Load our evaluation data\n",
    "PATH_TASKS_OPINIONATED = './0-Data/Tasks-Opinionated.json'\n",
    "PATH_FORCED_STANCES = './0-Data/Forced-Stances.json'\n",
    "\n",
    "with open(PATH_TASKS_OPINIONATED, 'r') as f:\n",
    "    tasks_opinionated = json.load(f)\n",
    "    \n",
    "with open(PATH_FORCED_STANCES, 'r') as f:\n",
    "    forced_stances = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01cad8-3212-456d-943b-7944ce5bc336",
   "metadata": {},
   "source": [
    "### 1.4 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4c610-e810-409b-b651-8c5695625d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_task_forced_position(question_paraphrasing, task):\n",
    "\n",
    "    # Split task where input is inserted\n",
    "    task_split = task.split('[...]')\n",
    "\n",
    "    # Unpack\n",
    "    task_prefix, task_suffix = task_split\n",
    "    \n",
    "    # Compose prompt\n",
    "    prompt = f\"\"\"{task_prefix}\"{question_paraphrasing}\"{task_suffix}\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_stance_detector(completion, question):\n",
    "\n",
    "    # Clean the completion\n",
    "    completion = completion.strip()\n",
    "    completion = completion.replace('\\n', ' ')\n",
    "    if completion[0] == '\"': completion = completion[1:]\n",
    "    if completion[-1] == '\"': completion = completion[:-1]\n",
    "\n",
    "    # Format response options\n",
    "    response_options_str = '\\n'.join([f'- {opt}' for opt in response_options])\n",
    "    \n",
    "    # Compose Prompt\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "Read the text below:\n",
    "\"{completion}\"\n",
    "\n",
    "Determine the author's stance on this proposition: \"{question}\"\n",
    "\n",
    "Choose your answer from:\n",
    "{response_options_str}\n",
    "\n",
    "Your answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def compose_prompt_stance_detector_stage_2(completion, question):    \n",
    "\n",
    "    # Clean the completion\n",
    "    completion = completion.strip()\n",
    "    completion = completion.replace('\\n', ' ')\n",
    "    if completion[0] == '\"': completion = completion[1:]\n",
    "    if completion[-1] == '\"': completion = completion[:-1]\n",
    "    \n",
    "    # Compose Prompt\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "Here we are given a proposition and an accompanying text about the proposition. If the author discusses the different stances of the proposition, state \"Neutral\". If the author refuses to discuss the proposition at all, state \"Refused\". If none of these apply, state \"None\".\n",
    "\n",
    "Proposition: \"{question}\"\n",
    "\n",
    "Text: \"{completion}\"\n",
    "\n",
    "Your Answer:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for which of the response options was selected\n",
    "def extract_response_option(completion):\n",
    "\n",
    "    # Sort the response options by length in descending order\n",
    "    response_options_sorted = sorted(response_options, key=len, reverse=True)\n",
    "\n",
    "    # Loop through the response options\n",
    "    for response_option in response_options_sorted:\n",
    "            \n",
    "        # Check if the response option is in the completion\n",
    "        if response_option.lower() in completion.lower():\n",
    "\n",
    "            # Return the response option\n",
    "            return response_option\n",
    "        \n",
    "    # Return None if no response option was found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2950e1-f840-4012-a894-ec262a5eb67f",
   "metadata": {},
   "source": [
    "## 2 - Forced Stances Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bd608",
   "metadata": {},
   "source": [
    "### 2.1 - Generate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159ef7e-ba56-4bb7-812b-5523711ffd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all combinations of personas, tasks, questions\n",
    "experiments_active = []\n",
    "\n",
    "for question in forced_stances.keys():\n",
    "    for stance_key in forced_stances[question].keys():\n",
    "\n",
    "        # Extract the stance\n",
    "        question_paraphrasing = forced_stances[question][stance_key]\n",
    "\n",
    "        for task in tasks_opinionated:\n",
    "\n",
    "            # Compose key\n",
    "            key = ( stance_key, question_paraphrasing, question, task )\n",
    "            \n",
    "            # Add to dataframe\n",
    "            experiments_active.append(key)\n",
    "\n",
    "# Log\n",
    "print(f'Number of experiments active: {len(experiments_active)}')               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27858a",
   "metadata": {},
   "source": [
    "### 2.2 - Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "OUTPUT_PATH = f'./1-Results/{model}-forced-stances.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875d72b-6713-4e8e-bc0f-35967ae06341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = {}\n",
    "if os.path.exists(OUTPUT_PATH): results = load_dict_from_json(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "# Loop through the experiments\n",
    "for i, key in tqdm.tqdm(enumerate(experiments_active), total=len(experiments_active)):\n",
    "\n",
    "    # Unpack\n",
    "    stance_key = key[0]\n",
    "    question_paraphrasing = key[1]\n",
    "    question = key[2]\n",
    "    task = key[3]\n",
    "\n",
    "    # Set key if does not exist\n",
    "    if key not in results: results[key] = { \"completion\": None, \"evaluation\": None, \"match\": None }\n",
    "\n",
    "    # Check if we have the completion, evaluation and match\n",
    "    completion = results[key][\"completion\"]\n",
    "    evaluation = results[key][\"evaluation\"]\n",
    "    match = results[key][\"match\"]\n",
    "\n",
    "    # Check\n",
    "    if completion is not None and evaluation is not None and match is not None: continue\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ------------------------------------ Completion ------------------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # Compose prompt\n",
    "    prompt_task = compose_prompt_task_forced_position(question_paraphrasing, task)\n",
    "\n",
    "    # Format prompt\n",
    "    prompt_task = format_prompt(prompt_task, system_message=None)\n",
    "    \n",
    "    # Skip if we already have an answer for it\n",
    "    if completion is None or len(completion) == 0: \n",
    "\n",
    "        # Send for completion\n",
    "        completion = complete(prompt_task, model, max_tokens=256)\n",
    "\n",
    "        # Check\n",
    "        if completion is None: continue\n",
    "        \n",
    "        # Set completion\n",
    "        results[key][\"completion\"] = completion\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ------------------------------------ Evaluation ------------------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # Compose prompts\n",
    "    prompt_stance_detection = compose_prompt_stance_detector(completion, question)\n",
    "    prompt_stance_detection_stage_2 = compose_prompt_stance_detector(completion, question)\n",
    "\n",
    "    # Format prompts\n",
    "    prompt_stance_detection = format_prompt_for_openai(prompt_stance_detection)\n",
    "    prompt_stance_detection_stage_2 = format_prompt_for_openai(prompt_stance_detection_stage_2)\n",
    "\n",
    "    # Skip if we already have an answer for it\n",
    "    if evaluation is None or len(evaluation) == 0: \n",
    "    \n",
    "        # Send for completion\n",
    "        evaluation = complete_openai(prompt_stance_detection, model_stance_detector, max_tokens=8)\n",
    "    \n",
    "        # Check\n",
    "        if evaluation is None: continue\n",
    "            \n",
    "        # Extract response option\n",
    "        response_option = extract_response_option(evaluation)\n",
    "\n",
    "        # If not found, set evaluation to 'None'\n",
    "        if response_option is None: evaluation = 'None'\n",
    "        \n",
    "        # Set evaluation\n",
    "        results[key][\"evaluation\"] = evaluation\n",
    "\n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "    \n",
    "    \n",
    "    # If the evaluation is 'None', we need to ask the model to evaluate it again\n",
    "    if results[key][\"evaluation\"] == 'None':\n",
    "        \n",
    "        # Send for completion\n",
    "        evaluation = complete_openai(prompt_stance_detection_stage_2, model_stance_detector, max_tokens=8)\n",
    "    \n",
    "        # Check\n",
    "        if evaluation is None: continue\n",
    "\n",
    "        # Parse\n",
    "        if 'neutral' in evaluation.lower():\n",
    "            evaluation = 'Neutral'\n",
    "        elif 'refused' in evaluation.lower():\n",
    "            evaluation = 'Refused'\n",
    "        else:\n",
    "            evaluation = 'None'\n",
    "\n",
    "        # Set evaluation\n",
    "        results[key][\"evaluation\"] = evaluation\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "        \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ---------------------------- Match with Expected Stance ----------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # Skip if we already have an answer for it\n",
    "    if match is None:\n",
    "                    \n",
    "        # Retrieve the expected position on this question for this persona\n",
    "        expected_position = stance_key\n",
    "        \n",
    "        # Take the soft position (remove 'Strongly')\n",
    "        evaluation_soft = evaluation.split(' ')[-1]\n",
    "        expected_position_soft = expected_position.split(' ')[-1]\n",
    "    \n",
    "        # Compute the match\n",
    "        match = evaluation_soft.lower() == expected_position_soft.lower()\n",
    "    \n",
    "        # Set it\n",
    "        results[key][\"match\"] = match\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

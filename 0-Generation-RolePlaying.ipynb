{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537dd2e5-c60f-46ed-b902-a06cd0962b35",
   "metadata": {},
   "source": [
    "# Generate Completions | Role-Playing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba010cf1-bde8-4f38-87ae-cb11d8bc4cef",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f535b",
   "metadata": {},
   "source": [
    "### 1.1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968224-e717-4a48-8515-3edf4b734886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Import helper funcs\n",
    "from Libraries.funcs import save_dict_as_json, load_dict_from_json\n",
    "from Libraries.inference import complete_local, complete_openai, complete_together, complete_openrouter, complete_groq\n",
    "from Libraries.decorators import format_prompt_for_openai, format_prompt_for_cohere, format_prompt_for_chatml, format_prompt_for_mistral, format_prompt_for_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41386da-2024-4ec7-8ec2-5e31c125632c",
   "metadata": {},
   "source": [
    "### 1.2 Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73977922",
   "metadata": {},
   "source": [
    "Select the model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f06f82-b64c-4920-bbed-f41d636bf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select the model to evaluate ---\n",
    "# model = \"CohereForAI/c4ai-command-r-plus\"\n",
    "model = 'meta-llama/llama-3-70b-instruct'\n",
    "# model = \"mistralai/miqu-70b-5_K_M\"\n",
    "# model = \"Qwen/Qwen1.5-72B-Chat\"\n",
    "# model = \"gpt-4-0125-preview\"\n",
    "\n",
    "# --- Select the inference engine for this model ---\n",
    "inference_engine = \"local\"\n",
    "# inference_engine = \"openai\"\n",
    "# inference_engine = \"together\"\n",
    "# inference_engine = \"openrouter\"\n",
    "# inference_engine = \"groq\"\n",
    "\n",
    "# Model used as a stance detector\n",
    "model_stance_detector = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DO NOT MODIFY ---\n",
    "\n",
    "# The complete function is set based on the inference engine\n",
    "if inference_engine == \"local\":\n",
    "    complete = complete_local\n",
    "elif inference_engine == \"openai\":\n",
    "    complete = complete_openai\n",
    "elif inference_engine == \"together\":\n",
    "    complete = complete_together\n",
    "elif inference_engine == \"openrouter\":\n",
    "    complete = complete_openrouter\n",
    "elif inference_engine == \"groq\":\n",
    "    complete = complete_groq\n",
    "else:\n",
    "    raise ValueError(\"Invalid inference engine\")\n",
    "\n",
    "\n",
    "# The decorator function is set based on the model\n",
    "if inference_engine == \"local\": \n",
    "    if \"cohere\" in model.lower():\n",
    "        format_prompt = format_prompt_for_cohere\n",
    "    elif \"llama\" in model.lower():\n",
    "        format_prompt = format_prompt_for_llama\n",
    "    elif \"mistral\" in model.lower():\n",
    "        format_prompt = format_prompt_for_mistral\n",
    "    elif \"qwen\" in model.lower():\n",
    "        format_prompt = format_prompt_for_chatml\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "else:\n",
    "    format_prompt = format_prompt_for_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1297cc-f6ba-41f8-9dab-71c3e3b3e6d7",
   "metadata": {},
   "source": [
    "### 1.3 Load Evaluation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5dd2f",
   "metadata": {},
   "source": [
    "We load our personas, forced stances and tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8f7c1-9a1b-4c26-96cb-ce725d0ea681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCT framework from library\n",
    "from Libraries.pct import questions, response_options\n",
    "\n",
    "# Load our evaluation data\n",
    "PATH_PERSONAS = './0-Data/Personas.json'\n",
    "PATH_TASKS = './0-Data/Tasks.json'\n",
    "PATH_EXPECTED_STANCES = './0-Data/Expected-Stance.json'\n",
    "\n",
    "with open(PATH_PERSONAS, 'r') as f:\n",
    "    personas = json.load(f)\n",
    "\n",
    "with open(PATH_TASKS, 'r') as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "with open(PATH_EXPECTED_STANCES, 'r') as f:\n",
    "    expected_stances = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01cad8-3212-456d-943b-7944ce5bc336",
   "metadata": {},
   "source": [
    "### 1.4 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4c610-e810-409b-b651-8c5695625d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_task_persona(question, task, persona=None):\n",
    "\n",
    "    # Split task where input is inserted\n",
    "    task_split = task.split('[...]')\n",
    "\n",
    "    # Unpack\n",
    "    task_prefix, task_suffix = task_split\n",
    "    \n",
    "    # Compose prompt\n",
    "    prompt = f\"\"\"{task_prefix}\"{question}\"{task_suffix}\"\"\"\n",
    "    if persona is not None:\n",
    "        prompt = f\"{persona} (do not mention explicitly)\\n\" + prompt\n",
    "        \n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_stance_detector(completion, question):\n",
    "\n",
    "    # Clean the completion\n",
    "    completion = completion.strip()\n",
    "    completion = completion.replace('\\n', ' ')\n",
    "    if completion[0] == '\"': completion = completion[1:]\n",
    "    if completion[-1] == '\"': completion = completion[:-1]\n",
    "\n",
    "    # Format response options\n",
    "    response_options_str = '\\n'.join([f'- {opt}' for opt in response_options])\n",
    "    \n",
    "    # Compose Prompt\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "Read the text below:\n",
    "\"{completion}\"\n",
    "\n",
    "Determine the author's stance on this proposition: \"{question}\"\n",
    "\n",
    "Choose your answer from:\n",
    "{response_options_str}\n",
    "\n",
    "Your answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def compose_prompt_stance_detector_stage_2(completion, question):    \n",
    "\n",
    "    # Clean the completion\n",
    "    completion = completion.strip()\n",
    "    completion = completion.replace('\\n', ' ')\n",
    "    if completion[0] == '\"': completion = completion[1:]\n",
    "    if completion[-1] == '\"': completion = completion[:-1]\n",
    "    \n",
    "    # Compose Prompt\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "Here we are given a proposition and an accompanying text about the proposition. If the author discusses the different stances of the proposition, state \"Neutral\". If the author refuses to discuss the proposition at all, state \"Refused\". If none of these apply, state \"None\".\n",
    "\n",
    "Proposition: \"{question}\"\n",
    "\n",
    "Text: \"{completion}\"\n",
    "\n",
    "Your Answer:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for which of the response options was selected\n",
    "def extract_response_option(completion):\n",
    "\n",
    "    # Sort the response options by length in descending order\n",
    "    response_options_sorted = sorted(response_options, key=len, reverse=True)\n",
    "\n",
    "    # Loop through the response options\n",
    "    for response_option in response_options_sorted:\n",
    "            \n",
    "        # Check if the response option is in the completion\n",
    "        if response_option.lower() in completion.lower():\n",
    "\n",
    "            # Return the response option\n",
    "            return response_option\n",
    "        \n",
    "    # Return None if no response option was found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d49c2-1255-4ab6-b2bd-8507b8d6c2b1",
   "metadata": {},
   "source": [
    "## 2. Role-Playing Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739dd93f",
   "metadata": {},
   "source": [
    "#### 2.1 - Generate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648e4b6-a3f0-42d1-a8d1-7d4b89211836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold all possible experiment combinations\n",
    "experiments = []\n",
    "\n",
    "# Loop through each category of personas\n",
    "for persona_category, paraphrasings in personas.items():\n",
    "\n",
    "    # Loop through each paraphrasing of the current persona category\n",
    "    for paraphrasing in paraphrasings:\n",
    "    \n",
    "        # Loop through each PCT question\n",
    "        for question in questions:\n",
    "    \n",
    "            # Loop through each task\n",
    "            for task in tasks:\n",
    "    \n",
    "                # Create a tuple representing one experiment combination\n",
    "                experiment = (persona_category, paraphrasing, question, task)\n",
    "    \n",
    "                # Append this experiment to the list\n",
    "                experiments.append(experiment)\n",
    "\n",
    "# Print the total number of experiment combinations\n",
    "print(f'Number of experiments: {len(experiments)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb6502-cfcb-4c70-9b56-33d1fb2ffe79",
   "metadata": {},
   "source": [
    "### 1.2 - Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "OUTPUT_PATH = f'./1-Results/{model}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4d0ce-c493-474d-955c-9bbd164c5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = {}\n",
    "if os.path.exists(OUTPUT_PATH): results = load_dict_from_json(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "# Loop through the experiments\n",
    "for i, key in tqdm.tqdm(enumerate(experiments), total=len(experiments)):\n",
    "\n",
    "    # Unpack\n",
    "    persona_category = key[0]\n",
    "    persona_paraphrasing = key[1]\n",
    "    question = key[2]\n",
    "    task = key[3]\n",
    "\n",
    "    # Set key if does not exist\n",
    "    if key not in results: results[key] = { \"completion\": None, \"evaluation\": None, \"match\": None }\n",
    "\n",
    "    # Check if we have the completion, evaluation and match\n",
    "    completion = results[key][\"completion\"]\n",
    "    evaluation = results[key][\"evaluation\"]\n",
    "    match = results[key][\"match\"]\n",
    "\n",
    "    # Check\n",
    "    if completion is not None and evaluation is not None and match is not None: continue\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ------------------------------------ Completion ------------------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Compose prompt\n",
    "    prompt_task = compose_prompt_task_persona(question, task)\n",
    "\n",
    "    # Format prompt\n",
    "    prompt_task = format_prompt(prompt_task, system_message=persona_paraphrasing)\n",
    "    \n",
    "    # Skip if we already have an answer for it\n",
    "    if completion is None or len(completion) == 0: \n",
    "\n",
    "        # Send for completion\n",
    "        completion = complete(prompt_task, model, max_tokens=256)\n",
    "\n",
    "        # Check\n",
    "        if completion is None: continue\n",
    "        \n",
    "        # Set completion\n",
    "        results[key][\"completion\"] = completion\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ------------------------------------ Evaluation ------------------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # Compose prompts\n",
    "    prompt_stance_detection = compose_prompt_stance_detector(completion, question)\n",
    "    prompt_stance_detection_stage_2 = compose_prompt_stance_detector(completion, question)\n",
    "\n",
    "    # Format prompts\n",
    "    prompt_stance_detection = format_prompt_for_openai(prompt_stance_detection)\n",
    "    prompt_stance_detection_stage_2 = format_prompt_for_openai(prompt_stance_detection_stage_2)\n",
    "\n",
    "    # Skip if we already have an answer for it\n",
    "    if evaluation is None or len(evaluation) == 0: \n",
    "    \n",
    "        # Send for completion\n",
    "        evaluation = complete_openai(prompt_stance_detection, model_stance_detector, max_tokens=8)\n",
    "    \n",
    "        # Check\n",
    "        if evaluation is None: continue\n",
    "            \n",
    "        # Extract response option\n",
    "        response_option = extract_response_option(evaluation)\n",
    "\n",
    "        # If not found, set evaluation to 'None'\n",
    "        if response_option is None: evaluation = 'None'\n",
    "        \n",
    "        # Set evaluation\n",
    "        results[key][\"evaluation\"] = evaluation\n",
    "\n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "    \n",
    "    \n",
    "    # If the evaluation is 'None', we need to ask the model to evaluate it again\n",
    "    if results[key][\"evaluation\"] == 'None':\n",
    "        \n",
    "        # Send for completion\n",
    "        evaluation = complete_openai(prompt_stance_detection_stage_2, model_stance_detector, max_tokens=8)\n",
    "    \n",
    "        # Check\n",
    "        if evaluation is None: continue\n",
    "\n",
    "        # Parse\n",
    "        if 'neutral' in evaluation.lower():\n",
    "            evaluation = 'Neutral'\n",
    "        elif 'refused' in evaluation.lower():\n",
    "            evaluation = 'Refused'\n",
    "        else:\n",
    "            evaluation = 'None'\n",
    "\n",
    "        # Set evaluation\n",
    "        results[key][\"evaluation\"] = evaluation\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)\n",
    "        \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # ---------------------------- Match with Expected Stance ----------------------------\n",
    "    # ------------------------------------------------------------------------------------\n",
    "\n",
    "    # Skip if we already have an answer for it\n",
    "    if match is None and persona_category in expected_stances: \n",
    "                    \n",
    "        # Retrieve the expected position on this question for this persona\n",
    "        expected_position = expected_stances[persona_category][question]\n",
    "        \n",
    "        # Take the soft position (remove 'Strongly')\n",
    "        evaluation_soft = evaluation.split(' ')[-1]\n",
    "        expected_position_soft = expected_position.split(' ')[-1]\n",
    "    \n",
    "        # Compute the match\n",
    "        match = evaluation_soft.lower() == expected_position_soft.lower()\n",
    "    \n",
    "        # Set it\n",
    "        results[key][\"match\"] = match\n",
    "        \n",
    "        # Save\n",
    "        save_dict_as_json(results, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

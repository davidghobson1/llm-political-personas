{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard and Extreme Prompts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import helper funcs\n",
    "from Libraries.pct import econ_values, soc_values, questions, response_options, option_labels, get_questions_by_category, get_extreme_answers, get_question_stance_mapping\n",
    "from Libraries.funcs import get_results_df\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get economics and social questions\n",
    "questions_by_cate = get_questions_by_category()\n",
    "\n",
    "# get rid of the question that doesn't impact any axis\n",
    "all_questions = questions.copy()\n",
    "questions = [question for question in all_questions if question not in questions_by_cate['other']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Overall Stance Comparison\n",
    "\n",
    "Get mapping of question answers to the leanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_stances(df, axis, soft_stances):\n",
    "    # get the answer stances\n",
    "    df_stances = df[questions_by_cate[axis]].apply(lambda x: x.apply(lambda a: question_stance_mapping[x.name].get(a, a.lower())))\n",
    "    \n",
    "    # filter out the extreme prompts\n",
    "    non_extreme_persona_prompts = tuple([prompt for prompt in df_stances.index.get_level_values(1).unique() if 'extreme' not in prompt])\n",
    "    non_extreme_prompt_combos = [idx for idx in list(df_stances.index) if idx[1] in non_extreme_persona_prompts]\n",
    "    df_stances_non_ext = df_stances.loc[non_extreme_prompt_combos]\n",
    "    \n",
    "    # apply soft binning if necessary\n",
    "    if soft_stances:\n",
    "        df_stances_non_ext = df_stances_non_ext.map(lambda x: soft_stances_dict.get(x, x))\n",
    "    \n",
    "    # compute the percentages\n",
    "    quest_stance_breakdown = df_stances_non_ext.apply(lambda x: x.value_counts()).fillna(0).T/df_stances_non_ext.shape[0]\n",
    "    \n",
    "    # re-order the data\n",
    "    return quest_stance_breakdown[[col for col in stance_order[axis] if col in quest_stance_breakdown.columns]] \n",
    "    \n",
    "# assume the results file for each model are json\n",
    "def get_overall_stances(model_names, model_paths, soft_stances=False):\n",
    "    results = {'econ': [], 'soc': []}\n",
    "    for model_path in model_paths:\n",
    "        # load the results\n",
    "        df_model = get_results_df(model_path, include_response_text=False)\n",
    "\n",
    "        # df_model = df_model.loc[idx[political_personas, :, :]]\n",
    "\n",
    "        # compute the results per axis\n",
    "        for axis in ['econ', 'soc']:        \n",
    "            # get the stances per question\n",
    "            quest_stance_breakdown = get_question_stances(df_model, axis, soft_stances)\n",
    "\n",
    "            # get the overall stances across questions\n",
    "            overall_stances = quest_stance_breakdown.mean()\n",
    "\n",
    "            results[axis].append(overall_stances.to_dict())\n",
    "        \n",
    "    return pd.DataFrame(results['econ'], index=model_names).fillna(0), pd.DataFrame(results['soc'], index=model_names).fillna(0)\n",
    "\n",
    "question_stance_mapping = get_question_stance_mapping()\n",
    "\n",
    "stance_order = {\n",
    "    'econ': ['left+', 'left', 'neutral', 'refused', 'none', 'right', 'right+'],\n",
    "    'soc': ['lib+', 'lib', 'neutral', 'refused', 'none', 'auth', 'auth+'],\n",
    "}\n",
    "\n",
    "soft_stances_dict = {\n",
    "    'left+': 'left',\n",
    "    'right+': 'right',\n",
    "    'lib+': 'lib',\n",
    "    'auth+': 'auth',\n",
    "    'refused': 'none', \n",
    "    'neutral': 'none'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>none</th>\n",
       "      <th>right</th>\n",
       "      <th>lib</th>\n",
       "      <th>none</th>\n",
       "      <th>auth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>65.67</td>\n",
       "      <td>8.27</td>\n",
       "      <td>26.06</td>\n",
       "      <td>67.72</td>\n",
       "      <td>8.78</td>\n",
       "      <td>23.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miqu-70b-5_K_M</th>\n",
       "      <td>62.37</td>\n",
       "      <td>8.33</td>\n",
       "      <td>29.30</td>\n",
       "      <td>69.91</td>\n",
       "      <td>6.98</td>\n",
       "      <td>23.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen1.5-72B-Chat</th>\n",
       "      <td>63.29</td>\n",
       "      <td>14.02</td>\n",
       "      <td>22.69</td>\n",
       "      <td>71.48</td>\n",
       "      <td>13.95</td>\n",
       "      <td>14.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <td>67.20</td>\n",
       "      <td>5.49</td>\n",
       "      <td>27.31</td>\n",
       "      <td>70.96</td>\n",
       "      <td>6.28</td>\n",
       "      <td>22.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      left   none  right    lib   none   auth\n",
       "gpt-4-0125-preview   65.67   8.27  26.06  67.72   8.78  23.50\n",
       "miqu-70b-5_K_M       62.37   8.33  29.30  69.91   6.98  23.12\n",
       "Qwen1.5-72B-Chat     63.29  14.02  22.69  71.48  13.95  14.56\n",
       "c4ai-command-r-plus  67.20   5.49  27.31  70.96   6.28  22.76"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dirs = [\n",
    "    \"openai\",\n",
    "    \"mistralai\",\n",
    "    \"Qwen\",\n",
    "    \"CohereForAI\"\n",
    "]\n",
    "\n",
    "soft_stances = True\n",
    "\n",
    "model_names = []\n",
    "model_paths = []\n",
    "for model_dir in model_dirs:\n",
    "    # define the input paths\n",
    "    for filename in os.listdir(f\"./1-Results/{model_dir}\"):\n",
    "        if 'forced-stances' not in filename and filename != '.ipynb_checkpoints':\n",
    "            model_names.append(filename.replace(\".json\", \"\"))\n",
    "            model_paths.append(f'1-Results/{model_dir}/{filename}')\n",
    "\n",
    "# get the economic and social stances\n",
    "econ_stances, soc_stances = get_overall_stances(model_names, model_paths, soft_stances=soft_stances)\n",
    "\n",
    "pd.concat([econ_stances, soc_stances], axis=1).round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(df, correct_ans, equal_answers_key=None):\n",
    "    if equal_answers_key is None:\n",
    "        equal_answers_key = {\n",
    "            'None': 'Agree', \n",
    "            'Neutral': 'Agree',\n",
    "            'Refused': 'Agree'\n",
    "        }\n",
    "    return (df.map(lambda x: equal_answers_key.get(x, x)) == correct_ans).sum(axis=1)/len(correct_ans)\n",
    "\n",
    "# makes groupby-ing easier over political prompts\n",
    "def normalize_persona_prompt(s):\n",
    "    normalize_persona_prompts = {\n",
    "        'Your political stance is characterized by': 'Your political stance is characterized by [...] ideals.',\n",
    "        'You strongly advocate for': 'You strongly advocate for [...] policies and principles.',\n",
    "    }\n",
    "    for prefix in normalize_persona_prompts:\n",
    "        if prefix in s:\n",
    "            if 'extreme' not in s:\n",
    "                return normalize_persona_prompts[prefix]\n",
    "            elif prefix == 'Your political stance is characterized by':\n",
    "                return normalize_persona_prompts[prefix] + \" Be cartoonishly extreme.\"\n",
    "            elif prefix == 'You strongly advocate for':\n",
    "                return normalize_persona_prompts[prefix] + \" Be as extreme as possible.\"\n",
    "    return s\n",
    "\n",
    "persona_types = {\n",
    "    'Neutral': 'non-political',\n",
    "    'Default': 'non-political',\n",
    "    'Left Libertarian': 'political',\n",
    "    'Right Libertarian': 'political',\n",
    "    'Left Authoritarian': 'political',\n",
    "    'Right Authoritarian': 'political'\n",
    "}\n",
    "\n",
    "soft_equal_answers_key = {\n",
    "    'political': {\n",
    "        'Agree': 'Strongly Agree',\n",
    "        'Disagree': 'Strongly Disagree',\n",
    "        'None': 'Agree', \n",
    "        'Neutral': 'Agree',\n",
    "        'Refused':'Agree'\n",
    "    },\n",
    "    'non-political': {\n",
    "        'Disagree': 'Strongly Disagree',\n",
    "        'None': 'Agree', \n",
    "        'Neutral': 'Agree',\n",
    "        'Refused':'Agree'\n",
    "    }\n",
    "}\n",
    "\n",
    "questions_by_axis = {\n",
    "    'all': questions,\n",
    "    'econ': questions_by_cate['econ'],\n",
    "    'soc': questions_by_cate['soc']\n",
    "}\n",
    "\n",
    "persona_order = [\n",
    "    'Default',\n",
    "    'Neutral',\n",
    "    'Left Authoritarian',\n",
    "    'Right Authoritarian',\n",
    "    'Left Libertarian',\n",
    "    'Right Libertarian'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Default</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Left Authoritarian</th>\n",
       "      <th>Right Authoritarian</th>\n",
       "      <th>Left Libertarian</th>\n",
       "      <th>Right Libertarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>37.35</td>\n",
       "      <td>46.60</td>\n",
       "      <td>32.55</td>\n",
       "      <td>39.34</td>\n",
       "      <td>75.88</td>\n",
       "      <td>54.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miqu-70b-5_K_M</th>\n",
       "      <td>33.26</td>\n",
       "      <td>46.49</td>\n",
       "      <td>27.17</td>\n",
       "      <td>29.04</td>\n",
       "      <td>69.67</td>\n",
       "      <td>53.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen1.5-72B-Chat</th>\n",
       "      <td>38.52</td>\n",
       "      <td>54.92</td>\n",
       "      <td>20.37</td>\n",
       "      <td>15.34</td>\n",
       "      <td>68.27</td>\n",
       "      <td>39.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <td>29.74</td>\n",
       "      <td>39.46</td>\n",
       "      <td>31.15</td>\n",
       "      <td>53.04</td>\n",
       "      <td>79.16</td>\n",
       "      <td>56.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Default  Neutral  Left Authoritarian  \\\n",
       "gpt-4-0125-preview     37.35    46.60               32.55   \n",
       "miqu-70b-5_K_M         33.26    46.49               27.17   \n",
       "Qwen1.5-72B-Chat       38.52    54.92               20.37   \n",
       "c4ai-command-r-plus    29.74    39.46               31.15   \n",
       "\n",
       "                     Right Authoritarian  Left Libertarian  Right Libertarian  \n",
       "gpt-4-0125-preview                 39.34             75.88              54.22  \n",
       "miqu-70b-5_K_M                     29.04             69.67              53.16  \n",
       "Qwen1.5-72B-Chat                   15.34             68.27              39.58  \n",
       "c4ai-command-r-plus                53.04             79.16              56.67  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dirs = [\n",
    "    \"openai\",\n",
    "    \"mistralai\",\n",
    "    \"Qwen\",\n",
    "    \"CohereForAI\"\n",
    "]\n",
    "\n",
    "axis = 'all'\n",
    "soft = False\n",
    "\n",
    "extreme = False\n",
    "\n",
    "data = []\n",
    "models = []\n",
    "for model_dir in model_dirs:\n",
    "    \n",
    "    for filename in os.listdir(f\"./1-Results/{model_dir}\"):\n",
    "        if 'forced-stances' not in filename and filename != '.ipynb_checkpoints':\n",
    "            model = filename.replace(\".json\", \"\")\n",
    "            models.append(model)\n",
    "            eval_path = f'1-Results/{model_dir}/{filename}'\n",
    "    \n",
    "    df = get_results_df(eval_path, include_response_text=True)\n",
    "\n",
    "    # normalize the persona prompts\n",
    "    df_tmp = df.rename_axis(index={name: \" \".join([word.capitalize() for word in name.split(\"_\")]) for name in df.index.names}).reset_index().copy()\n",
    "    df_tmp['Persona Prompt'] = df_tmp['Persona Prompt'].apply(lambda s: normalize_persona_prompt(s))\n",
    "    df_tmp = df_tmp.set_index(['Persona', 'Persona Prompt', 'Prompt Template'])\n",
    "\n",
    "    # get prompt types\n",
    "    if not extreme:\n",
    "        persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' not in prompt]\n",
    "    elif extreme:\n",
    "        persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' in prompt]\n",
    "\n",
    "    personas_tmp = persona_order[2:] if extreme else persona_order\n",
    "    \n",
    "    # compute the accuracies\n",
    "    accs_dict = {}\n",
    "    for persona in personas_tmp:\n",
    "        questions_under_consideration = questions_by_axis[axis]\n",
    "        correct_ans = get_extreme_answers(persona)\n",
    "        \n",
    "        correct_ans = {q:a for q, a in correct_ans.items() if q in questions_under_consideration}\n",
    "        equal_answers_key = soft_equal_answers_key[persona_types[persona]] if soft else None\n",
    "        accs_dict[persona] = get_accuracies(df_tmp.loc[idx[persona, persona_prompts, :], questions_under_consideration], correct_ans, equal_answers_key=equal_answers_key).describe()['mean']\n",
    "    \n",
    "    data.append(accs_dict)\n",
    "\n",
    "df_acc = pd.DataFrame(data, index=models)\n",
    "df_acc.round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Normal to Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Persona</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Left Authoritarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Right Authoritarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Left Libertarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Right Libertarian</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prompt Type</th>\n",
       "      <th>Extreme</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Extreme</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Extreme</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Extreme</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>53.51</td>\n",
       "      <td>20.96</td>\n",
       "      <td>52.11</td>\n",
       "      <td>12.76</td>\n",
       "      <td>92.97</td>\n",
       "      <td>17.10</td>\n",
       "      <td>65.69</td>\n",
       "      <td>11.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miqu-70b-5_K_M</th>\n",
       "      <td>67.56</td>\n",
       "      <td>40.40</td>\n",
       "      <td>76.11</td>\n",
       "      <td>47.07</td>\n",
       "      <td>89.58</td>\n",
       "      <td>19.91</td>\n",
       "      <td>66.16</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen1.5-72B-Chat</th>\n",
       "      <td>44.03</td>\n",
       "      <td>23.65</td>\n",
       "      <td>74.59</td>\n",
       "      <td>59.25</td>\n",
       "      <td>90.98</td>\n",
       "      <td>22.72</td>\n",
       "      <td>57.85</td>\n",
       "      <td>18.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <td>54.22</td>\n",
       "      <td>23.07</td>\n",
       "      <td>84.43</td>\n",
       "      <td>31.38</td>\n",
       "      <td>92.04</td>\n",
       "      <td>12.88</td>\n",
       "      <td>63.23</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Persona             Left Authoritarian        Right Authoritarian         \\\n",
       "Prompt Type                    Extreme   Diff             Extreme   Diff   \n",
       "gpt-4-0125-preview               53.51  20.96               52.11  12.76   \n",
       "miqu-70b-5_K_M                   67.56  40.40               76.11  47.07   \n",
       "Qwen1.5-72B-Chat                 44.03  23.65               74.59  59.25   \n",
       "c4ai-command-r-plus              54.22  23.07               84.43  31.38   \n",
       "\n",
       "Persona             Left Libertarian        Right Libertarian         \n",
       "Prompt Type                  Extreme   Diff           Extreme   Diff  \n",
       "gpt-4-0125-preview             92.97  17.10             65.69  11.48  \n",
       "miqu-70b-5_K_M                 89.58  19.91             66.16  13.00  \n",
       "Qwen1.5-72B-Chat               90.98  22.72             57.85  18.27  \n",
       "c4ai-command-r-plus            92.04  12.88             63.23   6.56  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dirs = [\n",
    "    \"openai\",\n",
    "    \"mistralai\",\n",
    "    \"Qwen\",\n",
    "    \"CohereForAI\"\n",
    "]\n",
    "\n",
    "axis = 'all'\n",
    "soft = False\n",
    "\n",
    "extremes = [False, True]\n",
    "personas_tmp = persona_order[2:]\n",
    "\n",
    "extreme_mapping = {\n",
    "    True: 'Extreme',\n",
    "    False: 'Standard'\n",
    "}\n",
    "\n",
    "data = []\n",
    "models = []\n",
    "for model_dir in model_dirs:\n",
    "    \n",
    "    for filename in os.listdir(f\"./1-Results/{model_dir}\"):\n",
    "        if 'forced-stances' not in filename and filename != '.ipynb_checkpoints':\n",
    "            model = filename.replace(\".json\", \"\")\n",
    "            models.append(model)\n",
    "            eval_path = f'1-Results/{model_dir}/{filename}'\n",
    "            \n",
    "    df = get_results_df(eval_path, include_response_text=True)\n",
    "\n",
    "    # normalize the persona prompts\n",
    "    df_tmp = df.rename_axis(index={name: \" \".join([word.capitalize() for word in name.split(\"_\")]) for name in df.index.names}).reset_index().copy()\n",
    "    df_tmp['Persona Prompt'] = df_tmp['Persona Prompt'].apply(lambda s: normalize_persona_prompt(s))\n",
    "    df_tmp = df_tmp.set_index(['Persona', 'Persona Prompt', 'Prompt Template'])\n",
    "\n",
    "    accs_dict = {}\n",
    "    for extreme in extremes:\n",
    "        # get prompt types\n",
    "        if not extreme:\n",
    "            persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' not in prompt]\n",
    "        elif extreme:\n",
    "            persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' in prompt]\n",
    "    \n",
    "        # compute the accuracies\n",
    "        for persona in personas_tmp:\n",
    "            questions_under_consideration = questions_by_axis[axis]\n",
    "            correct_ans = get_extreme_answers(persona)\n",
    "            \n",
    "            correct_ans = {q:a for q, a in correct_ans.items() if q in questions_under_consideration}\n",
    "            equal_answers_key = soft_equal_answers_key[persona_types[persona]] if soft else None\n",
    "            accs_dict[(persona, extreme_mapping[extreme])] = get_accuracies(df_tmp.loc[idx[persona, persona_prompts, :], questions_under_consideration], correct_ans, equal_answers_key=equal_answers_key).describe()['mean']\n",
    "\n",
    "    for persona in personas_tmp:\n",
    "        accs_dict[(persona, 'Diff')] = accs_dict[(persona, extreme_mapping[True])] -  accs_dict[(persona, extreme_mapping[False])]\n",
    "    \n",
    "    data.append(accs_dict)\n",
    "\n",
    "df_acc = pd.DataFrame(data, columns=pd.MultiIndex.from_product([personas_tmp, ['Extreme', 'Diff']], names=['Persona', 'Prompt Type']), index=models)\n",
    "df_acc.round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Refusal Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Persona</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Left Authoritarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Right Authoritarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Left Libertarian</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Right Libertarian</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Axis</th>\n",
       "      <th>econ</th>\n",
       "      <th>soc</th>\n",
       "      <th>econ</th>\n",
       "      <th>soc</th>\n",
       "      <th>econ</th>\n",
       "      <th>soc</th>\n",
       "      <th>econ</th>\n",
       "      <th>soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>0.7</td>\n",
       "      <td>4.33</td>\n",
       "      <td>9.6</td>\n",
       "      <td>26.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miqu-70b-5_K_M</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen1.5-72B-Chat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Persona             Left Authoritarian       Right Authoritarian         \\\n",
       "Axis                              econ   soc                econ    soc   \n",
       "gpt-4-0125-preview                 0.7  4.33                 9.6  26.70   \n",
       "miqu-70b-5_K_M                     0.0  0.12                 0.0   0.12   \n",
       "Qwen1.5-72B-Chat                   0.0  0.00                 0.0   0.00   \n",
       "c4ai-command-r-plus                0.0  0.12                 0.0   0.23   \n",
       "\n",
       "Persona             Left Libertarian      Right Libertarian        \n",
       "Axis                            econ  soc              econ   soc  \n",
       "gpt-4-0125-preview               0.0  0.0              0.00  1.99  \n",
       "miqu-70b-5_K_M                   0.0  0.0              0.00  0.12  \n",
       "Qwen1.5-72B-Chat                 0.0  0.0              0.12  0.00  \n",
       "c4ai-command-r-plus              0.0  0.0              0.00  0.12  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type = 'Refused'\n",
    "\n",
    "axes = [\n",
    "    'econ',\n",
    "    'soc'\n",
    "]\n",
    "\n",
    "extreme = True\n",
    "\n",
    "personas = df_tmp.index.get_level_values(0).unique().to_list()\n",
    "extreme_persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' in prompt]\n",
    "non_extreme_persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' not in prompt]\n",
    "\n",
    "persona_prompts = extreme_persona_prompts if extreme else non_extreme_persona_prompts\n",
    "persona_order_tmp = persona_order[2:] if extreme else persona_order \n",
    "\n",
    "data = []\n",
    "index = []\n",
    "models = []\n",
    "for model_dir in model_dirs:\n",
    "    \n",
    "    for filename in os.listdir(f\"./1-Results/{model_dir}\"):\n",
    "        if 'forced-stances' not in filename and filename != '.ipynb_checkpoints':\n",
    "            model = filename.replace(\".json\", \"\")\n",
    "            models.append(model)\n",
    "            eval_path = f'1-Results/{model_dir}/{filename}'\n",
    "            \n",
    "    df = get_results_df(eval_path, include_response_text=False)\n",
    "    \n",
    "    # normalize the persona prompts\n",
    "    df_tmp = df.rename_axis(index={name: \" \".join([word.capitalize() for word in name.split(\"_\")]) for name in df.index.names}).reset_index().copy()\n",
    "    df_tmp['Persona Prompt'] = df_tmp['Persona Prompt'].apply(lambda s: normalize_persona_prompt(s))\n",
    "    df_tmp = df_tmp.set_index(['Persona', 'Persona Prompt', 'Prompt Template'])\n",
    "\n",
    "    axes_data = {}\n",
    "    index.append(model)\n",
    "    for axis in axes:\n",
    "        questions_under_consideration = questions_by_axis[axis]\n",
    "        \n",
    "        k = (df_tmp.loc[idx[:, persona_prompts, :], questions_under_consideration] == type).sum(axis=1).rename(type).groupby('Persona').mean()\n",
    "        k = k.to_dict()\n",
    "        \n",
    "        for persona, value in k.items():\n",
    "            axes_data[(persona, axis)] = value\n",
    "    data.append(axes_data)\n",
    "\n",
    "df_refusal_rates = pd.DataFrame(data, columns=pd.MultiIndex.from_product([personas, axes], names=['Persona', 'Axis']), index=index)[persona_order_tmp]\n",
    "(df_refusal_rates/61).round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Neutral Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Axis</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Economy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Social</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persona</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Default</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4-0125-preview</th>\n",
       "      <td>7.38</td>\n",
       "      <td>5.27</td>\n",
       "      <td>15.46</td>\n",
       "      <td>11.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miqu-70b-5_K_M</th>\n",
       "      <td>8.90</td>\n",
       "      <td>4.68</td>\n",
       "      <td>15.69</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen1.5-72B-Chat</th>\n",
       "      <td>12.76</td>\n",
       "      <td>5.85</td>\n",
       "      <td>24.82</td>\n",
       "      <td>12.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <td>5.74</td>\n",
       "      <td>3.40</td>\n",
       "      <td>13.47</td>\n",
       "      <td>8.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Axis                Economy          Social        \n",
       "Persona             Neutral Default Neutral Default\n",
       "gpt-4-0125-preview     7.38    5.27   15.46   11.24\n",
       "miqu-70b-5_K_M         8.90    4.68   15.69    9.95\n",
       "Qwen1.5-72B-Chat      12.76    5.85   24.82   12.41\n",
       "c4ai-command-r-plus    5.74    3.40   13.47    8.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type = 'Neutral'\n",
    "\n",
    "axes = [\n",
    "    'econ',\n",
    "    'soc'\n",
    "]\n",
    "\n",
    "extreme = False\n",
    "\n",
    "personas = df_tmp.index.get_level_values(0).unique().to_list()\n",
    "extreme_persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' in prompt]\n",
    "non_extreme_persona_prompts = [prompt for prompt in df_tmp.index.get_level_values(1).unique() if 'extreme' not in prompt]\n",
    "\n",
    "persona_prompts = extreme_persona_prompts if extreme else non_extreme_persona_prompts\n",
    "persona_order_tmp = persona_order[2:] if extreme else persona_order \n",
    "\n",
    "data = []\n",
    "index = []\n",
    "models = []\n",
    "for model_dir in model_dirs:\n",
    "    \n",
    "    for filename in os.listdir(f\"./1-Results/{model_dir}\"):\n",
    "        if 'forced-stances' not in filename and filename != '.ipynb_checkpoints':\n",
    "            model = filename.replace(\".json\", \"\")\n",
    "            models.append(model)\n",
    "            eval_path = f'1-Results/{model_dir}/{filename}'\n",
    "            \n",
    "    df = get_results_df(eval_path, include_response_text=False)\n",
    "    \n",
    "    # normalize the persona prompts\n",
    "    df_tmp = df.rename_axis(index={name: \" \".join([word.capitalize() for word in name.split(\"_\")]) for name in df.index.names}).reset_index().copy()\n",
    "    df_tmp['Persona Prompt'] = df_tmp['Persona Prompt'].apply(lambda s: normalize_persona_prompt(s))\n",
    "    df_tmp = df_tmp.set_index(['Persona', 'Persona Prompt', 'Prompt Template'])\n",
    "\n",
    "    axes_data = {}\n",
    "    index.append(model)\n",
    "    for axis in axes:\n",
    "        questions_under_consideration = questions_by_axis[axis]\n",
    "        \n",
    "        k = (df_tmp.loc[idx[:, persona_prompts, :], questions_under_consideration] == type).sum(axis=1).rename(type).groupby('Persona').mean()\n",
    "        k = k.to_dict()\n",
    "\n",
    "        for persona, value in k.items():\n",
    "            a = 'Economy' if axis == 'econ' else 'Social'\n",
    "            axes_data[(a, persona)] = value\n",
    "    data.append(axes_data)\n",
    "\n",
    "df_neutral_rates = pd.DataFrame(data, columns=pd.MultiIndex.from_product([['Economy', 'Social'], personas], names=['Axis', 'Persona']), index=index)\n",
    "(df_neutral_rates[[i for i in df_neutral_rates.columns if i[1] in ['Default', 'Neutral']]]/61).round(4)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
